{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×3的卷积层后接上一个步幅为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数。"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感受野和特征图\n",
    "二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素xx的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做xx的感受野（receptive field）。以图一1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图1中形状为2×2的输出记为Y，并考虑一个更深的卷积神经网络：将Y与另一个形状为2×2的核数组做互相关运算，输出单个元素z。那么，z在Y上的感受野包括Y的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "<center>图一</center>\n",
    "\n",
    "**VGG结构的特点:**\n",
    "\n",
    "对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    blk = []\n",
    "    for i in range(num_convs):\n",
    "        if i == 0:\n",
    "            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG简介\n",
    "与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。全连接模块则跟AlexNet中的一样。\n",
    "\n",
    "现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输入输出通道分别是1（因为下面要使用的Fashion-MNIST数据的通道数为1）和64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\n",
    "fc_features = 512 * 7 * 7 # c * w * h\n",
    "fc_hidden_units = 4096 # 任意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）构建VGG-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch, fc_features, fc_hidden_units=4096):\n",
    "    net = nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n",
    "        # 每经过一个vgg_block都会使宽高减半\n",
    "        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n",
    "    # 全连接层部分\n",
    "    net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(),\n",
    "                                 nn.Linear(fc_features, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, 10)\n",
    "                                ))\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）查看结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch, fc_features, fc_hidden_units)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从输出可以看到VGG-11的结构，又五个卷积快，和一个全连接层构成，前两个卷积快进行一次卷积之后进行激活和最大池化，后三个进行两次卷积和激活之后进行最大池化，所以卷积一共有8次，全连接层有三层连接，所以总共11层，所以叫VGG-11。我们下面看一下每一层后大小的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])\n",
      "vgg_block_2 output shape:  torch.Size([1, 128, 56, 56])\n",
      "vgg_block_3 output shape:  torch.Size([1, 256, 28, 28])\n",
      "vgg_block_4 output shape:  torch.Size([1, 512, 14, 14])\n",
      "vgg_block_5 output shape:  torch.Size([1, 512, 7, 7])\n",
      "fc output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch, fc_features, fc_hidden_units)\n",
    "X = torch.rand(1, 1, 224, 224)\n",
    "\n",
    "# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)\n",
    "for name, blk in net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为VGG-11计算上比AlexNet更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在Fashion-MNIST数据集上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ratio = 8\n",
    "small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), \n",
    "                   (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\n",
    "net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.4057, train acc 0.852, test acc 0.872, time 168.3 sec\n",
      "epoch 2, loss 0.1572, train acc 0.887, test acc 0.902, time 175.1 sec\n",
      "epoch 3, loss 0.0905, train acc 0.901, test acc 0.901, time 176.5 sec\n",
      "epoch 4, loss 0.0608, train acc 0.910, test acc 0.909, time 180.7 sec\n",
      "epoch 5, loss 0.0445, train acc 0.919, test acc 0.913, time 182.9 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
